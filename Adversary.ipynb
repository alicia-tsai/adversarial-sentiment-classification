{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from classifier import train_classifier, load_saved_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Data and Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data...\n",
      "building vocabulary...\n",
      "CNN(\n",
      "  (embedding): Embedding(25002, 100)\n",
      "  (convs1): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc1): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# this model is trained using large dataset\n",
    "cnn_model = load_saved_model('CNN', 'cnn-1.pt', data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_input(data_loader, k=1):\n",
    "    example = data_loader.large_train.examples[k].text\n",
    "    label = data_loader.large_train.examples[k].label\n",
    "    word_indices = np.array([data_loader.TEXT.vocab.stoi[word] for word in example])\n",
    "    one_input = torch.from_numpy(word_indices)\n",
    "        \n",
    "    return one_input.unsqueeze(1), label\n",
    "\n",
    "def get_logit(input_example, model, print_msg=False):\n",
    "    logit = model(input_example)\n",
    "    if print_msg:\n",
    "        print('logit:', logit)\n",
    "        print('pred:', torch.round(torch.sigmoid(logit)))\n",
    "    \n",
    "    return logit\n",
    "\n",
    "def get_predict(logit):\n",
    "    return torch.round(torch.sigmoid(logit))\n",
    "\n",
    "def generate_sentence(words_idx, data_loader):\n",
    "    sentence = ' '.join(data_loader.TEXT.vocab.itos[id] for id in words_idx)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Custom Loss Function and Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def custom_loss(new_logit, old_logit, new_word_vecs=None, initial_word_vecs=None, data_grad=torch.Tensor([0])):\n",
    "    loss = - F.mse_loss(new_logit, old_logit) + torch.sum(data_grad ** 2)\n",
    "    if new_word_vecs is not None and initial_word_vecs is not None:\n",
    "        loss += np.sum(np.square(list(map(cosine, new_word_vecs, initial_word_vecs))))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def attack(input_example, model, epsilon=1, similarity_reg=False, perturb_reg=False):\n",
    "    # input_example: 2D, tensor([1, number of words])\n",
    "    print('--- Initial ---')\n",
    "    initial_logit = get_logit(input_example, model, print_msg=True)\n",
    "    initial_label = get_predict(initial_logit)\n",
    "    new_logit = initial_logit.clone()\n",
    "    \n",
    "    # initial loss and backpropagation\n",
    "    loss = custom_loss(new_logit, initial_logit)\n",
    "    model.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    print('initial loss:', loss)\n",
    "\n",
    "    success = False\n",
    "    first_time = True\n",
    "    \n",
    "    print('\\n--- Attack ---')\n",
    "    while not success:\n",
    "        # get gradient and compute new embedding\n",
    "        data_grad = model.embedding.weight.grad[input_example.squeeze(0)].clone()\n",
    "        input_embedding = model.embedding.weight.data[input_example.squeeze(0)].clone()\n",
    "        perturbed_embedding = input_embedding - epsilon * data_grad\n",
    "        \n",
    "        new_words_idx = []\n",
    "        for i, one_embedding in enumerate(perturbed_embedding):\n",
    "            embedding_distance = torch.sum((one_embedding - model.embedding.weight.data) ** 2, dim=1)\n",
    "            # set original embedding distance to the maximum\n",
    "            embedding_distance[input_example.squeeze(0)[i]] = float('inf')\n",
    "\n",
    "            min_idx = torch.argmin(embedding_distance)\n",
    "            new_words_idx.append(min_idx)\n",
    "        \n",
    "        new_words_idx = torch.from_numpy(np.array(new_words_idx, dtype=int))   # 1D, tensor([number of words])\n",
    "        first_time = False\n",
    "        \n",
    "        # compute new logit and check if attack successfully\n",
    "        new_logit = get_logit(new_words_idx.unsqueeze(0), model, print_msg=True)\n",
    "        new_label = get_predict(new_logit)\n",
    "        \n",
    "        # compute loss\n",
    "        if perturb_reg and similarity_reg:\n",
    "            loss = custom_loss(new_logit, initial_logit, perturbed_embedding, input_embedding, data_grad)\n",
    "        elif similarity_reg:\n",
    "            loss = custom_loss(new_logit, initial_logit, perturbed_embedding, input_embedding)\n",
    "        else:\n",
    "            loss = custom_loss(new_logit, initial_logit)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        print('loss:', loss, '\\n')\n",
    "        \n",
    "        if new_label != initial_label:\n",
    "            break\n",
    "    \n",
    "    return new_words_idx, data_grad, new_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_adversarial(model, original_input, new_words_idx, data_grad, max_limit=5, print_msg=False):\n",
    "    initial_logit = get_logit(original_input, model)\n",
    "    initial_label = get_predict(initial_logit)\n",
    "    \n",
    "    # compute the magnitude of the perturb and change from the largest\n",
    "    grad_magnitude = torch.sqrt(torch.sum(torch.abs(data_grad), dim=1))\n",
    "    position_to_change = reversed(np.argsort(grad_magnitude))\n",
    "    \n",
    "    success = False\n",
    "    \n",
    "    print('--- Generate Adversary ---')\n",
    "    # changing words from the largest perturb\n",
    "    for i in range(1, len(position_to_change)):\n",
    "        new_input = original_input.squeeze(0).clone()\n",
    "        old_words, new_words = [], []\n",
    "        for position in position_to_change[:i]:\n",
    "            new_input[position] = new_words_idx[position]\n",
    "            old_words.append(data_loader.TEXT.vocab.itos[original_input.squeeze(0)[position]])\n",
    "            new_words.append(data_loader.TEXT.vocab.itos[new_words_idx[position]])\n",
    "\n",
    "        if print_msg:\n",
    "            print('\\nold words:', old_words)\n",
    "            print('new words:', new_words)\n",
    "        \n",
    "        new_logit = get_logit(new_input.unsqueeze(0), model, print_msg=print_msg)\n",
    "        new_label = get_predict(new_logit)\n",
    "        if new_label != initial_label:\n",
    "            success = True\n",
    "            break\n",
    "        \n",
    "        # change too many words\n",
    "        if i > max_limit:\n",
    "            break\n",
    "    \n",
    "    return success, new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4: 92 words. change all words to `not`\n",
    "- 9: 66 words. change `the` to `excellently` and `is` to `celebrate`\n",
    "- 10: 52 words. change all words to `not`\n",
    "- 14: 81 words. only change one word\n",
    "- 21: 62 words. change `<br />` tag\n",
    "- 22: 93 words. change `<br />` tage\n",
    "- 37: 44 words. replace `.` to `resilience`\n",
    "- 45: 38 words. change all words to `not`\n",
    "- 52: 61 words. change two words\n",
    "- 61: 59 words. change one word\n",
    "- 72: 92 words. fail. change all words to `not`\n",
    "- 87: 77 words. fail. change all words to `not`\n",
    "- 100: 83 words. change one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find examples that are less 100 words\n",
    "# for i in range(33, 101):\n",
    "#     one_input, _ = get_input(data_loader, k=i)\n",
    "#     one_input = torch.t(one_input)\n",
    "#     if one_input.shape[1] <= 100:\n",
    "#         print(i, one_input.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit: tensor([[1.0576]], grad_fn=<ThAddmmBackward>)\n",
      "pred: tensor([[1.]], grad_fn=<RoundBackward>)\n",
      "true label: ['pos']\n"
     ]
    }
   ],
   "source": [
    "# get one example\n",
    "one_input, one_label = get_input(data_loader, k=61)\n",
    "one_input = torch.t(one_input)\n",
    "\n",
    "logit = get_logit(one_input, cnn_model, print_msg=True)\n",
    "print('true label:', one_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial ---\n",
      "logit: tensor([[1.0576]], grad_fn=<ThAddmmBackward>)\n",
      "pred: tensor([[1.]], grad_fn=<RoundBackward>)\n",
      "initial loss: tensor(0., grad_fn=<ThAddBackward>)\n",
      "\n",
      "--- Attack ---\n",
      "logit: tensor([[-6.7423]], grad_fn=<ThAddmmBackward>)\n",
      "pred: tensor([[0.]], grad_fn=<RoundBackward>)\n",
      "loss: tensor(-60.8378, grad_fn=<AddBackward>) \n",
      "\n",
      "--- Generate Adversary ---\n",
      "\n",
      "old words: ['!']\n",
      "new words: ['yes']\n",
      "logit: tensor([[0.7841]], grad_fn=<ThAddmmBackward>)\n",
      "pred: tensor([[1.]], grad_fn=<RoundBackward>)\n",
      "\n",
      "old words: ['!', 'bad']\n",
      "new words: ['yes', 'worse']\n",
      "logit: tensor([[-2.5924]], grad_fn=<ThAddmmBackward>)\n",
      "pred: tensor([[0.]], grad_fn=<RoundBackward>)\n",
      "\n",
      "attack success: True\n"
     ]
    }
   ],
   "source": [
    "new_words_idx, data_grad, new_logit = attack(one_input, cnn_model, epsilon=1e-0, similarity_reg=True, perturb_reg=True)\n",
    "\n",
    "success, new_input = generate_adversarial(cnn_model, one_input, new_words_idx, data_grad, max_limit=10, print_msg=True)\n",
    "print('\\nattack success:', success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as long as you go into this movie knowing that it 's terrible : bad acting , bad \" effects , \" bad story , bad ... everything , then you 'll love it . this is one of my favorite \" goof on \" movies ; watch it as a comedy and have a dozen good laughs !\n"
     ]
    }
   ],
   "source": [
    "# original sentence\n",
    "print(generate_sentence(one_input.squeeze(0), data_loader).replace('<br />', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as long as you go into this movie knowing that it 's terrible : worse acting , bad \" effects , \" bad story , bad ... everything , then you 'll love it . this is one of my favorite \" goof on \" movies ; watch it as a comedy and have a dozen good laughs yes\n"
     ]
    }
   ],
   "source": [
    "# adversarial sentence\n",
    "print(generate_sentence(new_input, data_loader).replace('<br />', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
